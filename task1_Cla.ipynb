{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "CPIgz1GLB0h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag0Qr8AcCc_7",
        "outputId": "cc05e29f-3d35-4de8-cce2-c4ee1ca27987"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jjHH2ztBmfh",
        "outputId": "541268d0-fd04-4dd2-e852-5c87c28f917c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
        "from torchvision import datasets, transforms as T\n",
        "#import wandb\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the device and fixing the reproducibility"
      ],
      "metadata": {
        "id": "J10nUfL4CJao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "def fix_random(seed: int) -> None:\n",
        "    \"\"\"\n",
        "        Fix all the possible sources of randomness.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "fix_random(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1pWtCQqB8eM",
        "outputId": "0c70cfcb-9199-4924-83ef-958d20090aa8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and exploring dataset -> evaluate the question of balanced/unbalanced data"
      ],
      "metadata": {
        "id": "v8PDUd2VCwHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj0R7eZECvdL",
        "outputId": "4b1bce30-0510-43e2-a13b-7643cb2ad03a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroceryStoreDataset'...\n",
            "remote: Enumerating objects: 6559, done.\u001b[K\n",
            "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 6559 (delta 45), reused 35 (delta 35), pack-reused 6293 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6559/6559), 116.26 MiB | 15.15 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "5HC3ne7OCqPu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GroceryStoreDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split: str, transform=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.root = Path(\"GroceryStoreDataset/dataset\")\n",
        "        self.split = split\n",
        "        self.paths, self.labels = self.read_file()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img = Image.open(self.root / self.paths[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
        "        paths = []\n",
        "        labels = []\n",
        "\n",
        "        with open(self.root / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                # path, fine-grained class, coarse-grained class\n",
        "                path, _, label = line.replace(\"\\n\", \"\").split(\", \")\n",
        "                paths.append(path), labels.append(int(label))\n",
        "\n",
        "        return paths, labels\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1"
      ],
      "metadata": {
        "id": "MatJWVyvoNr2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Configurations\n",
        "\n",
        "cfg_data = {\n",
        "    'std_size' : 224,\n",
        "    'rand_crop_size' : 150,\n",
        "    'n_classes' : 43,\n",
        "}\n",
        "\n",
        "cfg_hyp = {\n",
        "    'lr' : 1e-2,\n",
        "    'wd' : 1e-5,\n",
        "    'epochs' : 50,\n",
        "    'batch_size' : 32,\n",
        "    'scheduler' : False,\n",
        "    'device' : device,\n",
        "    'stopping_patience' : 10,\n",
        "    'stopping_value' : 0.6\n",
        "}\n"
      ],
      "metadata": {
        "id": "lpES-4bwEh-0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Rimettere a 260x260 o a 224x224 (per resnet)\n",
        "\n",
        "n_classes = cfg_data['n_classes']\n",
        "\n",
        "tsfms_std = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Resize(size=(cfg_data['std_size'], cfg_data['std_size'])),\n",
        "    # T.Lambda(lambda x: x.flatten()),\n",
        "])\n",
        "\n",
        "#DATA AUGMENTATION\n",
        "#tsfms_increasing = T.Compose([\n",
        "#    T.RandomRotation(15),\n",
        "#    T.RandomResizedCrop(32, scale=(0.9, 1.1)),\n",
        "#    T.RandomHorizontalFlip(),\n",
        "#    T.RandomResizedCrop(size=(cfg_data['rand_crop_size'], cfg_data['rand_crop_size']), scale=(0.7, 0.9)),\n",
        "#    T.ToTensor(),\n",
        "#    T.Resize(size=(cfg_data['std_size'], cfg_data['std_size'])),\n",
        "#    # T.Lambda(lambda x: x.flatten()),\n",
        "#])\n",
        "\n",
        "\n",
        "#NEWDATAAUGMENTATION\n",
        "class RandomApply(T.RandomApply):\n",
        "    def __init__(self, transforms, p=0.5):\n",
        "        super().__init__(transforms, p=p)\n",
        "\n",
        "def change_background_color(image):\n",
        "    background = Image.new('RGB', image.size, (random.randint(200, 255), random.randint(200, 255), random.randint(200, 255)))\n",
        "    return Image.composite(image, background, image.convert('L').point(lambda x: 255 if x > 5 else 0))\n",
        "\n",
        "\n",
        "tsfms_increasing = T.Compose([\n",
        "        T.RandomRotation(10),\n",
        "        T.RandomResizedCrop(32, scale=(0.9, 1.0)),\n",
        "        T.RandomHorizontalFlip(p=0.3),\n",
        "        RandomApply([T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)], p=0.7),\n",
        "        RandomApply([T.GaussianBlur(kernel_size=3)], p=0.2),\n",
        "        T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        RandomApply([T.Lambda(change_background_color)], p=0.3),\n",
        "        T.RandomResizedCrop(size=(cfg_data['rand_crop_size'], cfg_data['rand_crop_size']), scale=(0.8, 1.0)),\n",
        "        T.ToTensor(),\n",
        "        T.Resize(size=(cfg_data['std_size'], cfg_data['std_size'])),\n",
        "        RandomApply([\n",
        "            T.Lambda(lambda x: x + 0.02 * torch.randn_like(x))\n",
        "        ], p=0.2),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "train_dset = GroceryStoreDataset(\n",
        "    split=\"train\",\n",
        "    transform=tsfms_std,\n",
        ")\n",
        "increased_train_dst = GroceryStoreDataset(\n",
        "    split=\"train\",\n",
        "    transform=tsfms_increasing,\n",
        ")\n",
        "\n",
        "val_dset = GroceryStoreDataset(\n",
        "    split=\"val\",\n",
        "    transform=tsfms_std,\n",
        ")\n",
        "test_dset = GroceryStoreDataset(\n",
        "    split=\"test\",\n",
        "    transform=tsfms_std,\n",
        ")\n",
        "increased_dataset = torch.utils.data.ConcatDataset([increased_train_dst,train_dset]) #-> augmented data\n",
        "\n",
        "print(f'Train dataset size: {len(train_dset)}')\n",
        "print(f'Increased train dataset size: {len(increased_dataset)}')\n",
        "print(f'Validation dataset size: {len(val_dset)}')\n",
        "print(f'Test dataset size: {len(test_dset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYUtT7mpWAA",
        "outputId": "4f8cd374-7a7a-43b3-e552-0c4a5ac464f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 2640\n",
            "Increased train dataset size: 5280\n",
            "Validation dataset size: 296\n",
            "Test dataset size: 2485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(\n",
        "    train_dset,\n",
        "    batch_size=cfg_hyp['batch_size'],\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "increased_train_dl = DataLoader(\n",
        "    increased_dataset,\n",
        "    batch_size=cfg_hyp['batch_size'],\n",
        "    shuffle=True\n",
        ")\n",
        "val_dl = DataLoader(\n",
        "    val_dset,\n",
        "    batch_size=cfg_hyp['batch_size']\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_dset,\n",
        "    batch_size=cfg_hyp['batch_size']\n",
        ")\n"
      ],
      "metadata": {
        "id": "TiNxnjfoil6C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What Can I have also do with the data is the balancing of them!"
      ],
      "metadata": {
        "id": "JO66-hD2l5o1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilty function"
      ],
      "metadata": {
        "id": "j45zpXZRGNI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_directory(model_name, run):\n",
        "    base_path = f\"/content/drive/MyDrive/model_results_{run}\"\n",
        "    model_path = os.path.join(base_path, model_name)\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    return model_path"
      ],
      "metadata": {
        "id": "JVhVloULS51Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "_8JkwbrjGPOD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name, model_path, stopping_patience = 10, stopping_value = 0.6, scheduler_check=False, WANDB=False):\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_acc = 0.0\n",
        "    if scheduler_check:\n",
        "      scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr = 1e-6)\n",
        "\n",
        "    if device == 'cuda':\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    num_params = count_parameters(model)\n",
        "\n",
        "    run_stats = {\n",
        "        \"model_name\": model_name,\n",
        "        \"num_parameters\": num_params,\n",
        "        \"epochs\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "        if scheduler_check:\n",
        "          scheduler.step(val_loss)\n",
        "          last_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        epoch_stats = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"epoch_time\": epoch_time\n",
        "        }\n",
        "\n",
        "        run_stats[\"epochs\"].append(epoch_stats)\n",
        "\n",
        "        if WANDB:\n",
        "          wandb.log(epoch_stats)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
        "              f\"Time: {epoch_time:.2f}s, \"\n",
        "              f\"Last leraning rate: {last_lr}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            early_stopping_counter = 0\n",
        "            torch.save(model.state_dict(), os.path.join(model_path, f\"{model_name}_best.pth\"))\n",
        "            if WANDB:\n",
        "              wandb.save(f\"{model_name}_best.pth\")\n",
        "\n",
        "        else:\n",
        "          early_stopping_counter += 1\n",
        "          print(f\"EarlyStoppingCounter: {early_stopping_counter} out of {stopping_patience}\")\n",
        "\n",
        "        if early_stopping_counter >= stopping_patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        if val_acc > stopping_value:\n",
        "            print(f\"Stopping value reached at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    with open(os.path.join(model_path, f\"{model_name}_run_stats.json\"), \"w\") as f:\n",
        "        json.dump(run_stats, f, indent=2)\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_and_evaluate(model_class, name, train_loader, val_loader, test_loader, num_classes, model_path, num_epochs=50, lr=0.001, wd=1e-5, device='cuda', stopping_patience = 10, stopping_value = 0.6, scheduler_check=False, WANDB=False):\n",
        "\n",
        "    model = model_class(num_classes=num_classes).to(device)\n",
        "    if WANDB:\n",
        "       wandb.init(project=\"grocery-store-classification\", name=name)\n",
        "       wandb.watch(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    best_val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, name, model_path, stopping_patience, stopping_value, scheduler_check, WANDB)\n",
        "\n",
        "    # Load the best model for final evaluation\n",
        "    model.load_state_dict(torch.load(os.path.join(model_path, f\"{name}_best.pth\")))\n",
        "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"Final results for {name}:\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    if WANDB:\n",
        "      wandb.log({\n",
        "          \"best_val_acc\": best_val_acc,\n",
        "          \"test_acc\": test_acc\n",
        "      })\n",
        "\n",
        "    json_path = os.path.join(model_path, f\"{name}_run_stats.json\")\n",
        "    with open(json_path, \"r\") as f:\n",
        "        run_stats = json.load(f)\n",
        "\n",
        "    run_stats[\"best_val_acc\"] = best_val_acc\n",
        "    run_stats[\"test_acc\"] = test_acc\n",
        "\n",
        "    with open(json_path, \"w\") as f:\n",
        "        json.dump(run_stats, f, indent=2)\n",
        "\n",
        "    if WANDB:\n",
        "      wandb.finish()\n",
        "\n",
        "\n",
        "    return best_val_acc, test_acc"
      ],
      "metadata": {
        "id": "6FSFlsp6ikIY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Networks"
      ],
      "metadata": {
        "id": "GPI3GyHQX0yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "28YYYZBliHMH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "##Tutte le reti tranne l'ultima non le ho testate, sono per l ablation study\n",
        "\n",
        "class MoreFullyConnectedNet(nn.Module):   #-> da vedere meglio come farla\n",
        "    def __init__(self, num_classes=43):\n",
        "        super(MoreFullyConnectedNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.linear(128,256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.relu(self.conv2(x))\n",
        "        #out = self.dropout1(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #out = self.dropout2(out)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class NetWithoutBatchNorm(nn.Module):\n",
        "    def __init__(self, num_classes=43):\n",
        "        super(NetWithoutBatchNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.relu(self.conv2(x))\n",
        "        out = F.relu(self.conv3(x))\n",
        "        out = F.relu(self.conv4(x))\n",
        "        #out = self.dropout1(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #out = self.dropout2(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class NetWithoutResidualBlock(nn.Module):\n",
        "    def __init__(self, num_classes=43):\n",
        "        super(NetWithoutResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(x)))\n",
        "        out = F.relu(self.bn3(self.conv3(x)))\n",
        "        out = F.relu(self.bn4(self.conv4(x)))\n",
        "        #out = self.dropout1(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        #out = self.dropout2(out)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class NetWihtoutDropout(nn.Module):\n",
        "    def __init__(self, num_classes=43):\n",
        "        super(NetWihtoutDropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = ResidualBlock(64, 64)\n",
        "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
        "        self.layer4 = ResidualBlock(256, 512, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "#BesttNet -> ResidualBlock + dropout best combination: best performance 0.58 with lr = 1e-3, wd = 1e-5 without scheduler -> da cambiare gli hyp ma ci siamo\n",
        "class GroceryConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=43):\n",
        "        super(GroceryConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = ResidualBlock(64, 64)\n",
        "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
        "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
        "        self.layer4 = ResidualBlock(256, 512, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        #out = self.dropout1(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout2(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "tc2u857-YoFn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Prova da non guardare\n",
        "model = GroceryConvNet\n",
        "name = 'ultimate_cnn'\n",
        "model_path = create_model_directory(model_name= name, run=1)\n",
        "best_val_acc, test_acc = train_and_evaluate(\n",
        "    model_class = model,\n",
        "    name = name,\n",
        "    train_loader = increased_train_dl,\n",
        "    val_loader = val_dl,\n",
        "    test_loader = test_dl,\n",
        "    num_classes = n_classes,\n",
        "    model_path = model_path,\n",
        "    num_epochs=cfg_hyp['epochs'],\n",
        "    lr=cfg_hyp['lr'],\n",
        "    wd=cfg_hyp['wd'],\n",
        "    device='cuda',\n",
        "    stopping_patience = cfg_hyp['stopping_patience'],\n",
        "    stopping_value = cfg_hyp['stopping_value'],\n",
        "    scheduler_check=True,\n",
        "    WANDB=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "_rVoWDOwjcK5",
        "outputId": "e10d7650-4bab-448c-8c4e-521449331509"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Train Loss: 3.4162, Train Acc: 0.1576, Val Loss: 3.1848, Val Acc: 0.2128, Time: 167.57s, Last leraning rate: 0.01\n",
            "Epoch 2/50, Train Loss: 3.0229, Train Acc: 0.2210, Val Loss: 3.0022, Val Acc: 0.2061, Time: 171.06s, Last leraning rate: 0.01\n",
            "EarlyStoppingCounter: 1 out of 10\n",
            "Epoch 3/50, Train Loss: 2.8957, Train Acc: 0.2564, Val Loss: 2.8538, Val Acc: 0.2973, Time: 171.44s, Last leraning rate: 0.01\n",
            "Epoch 4/50, Train Loss: 2.8246, Train Acc: 0.2828, Val Loss: 3.3135, Val Acc: 0.2534, Time: 171.27s, Last leraning rate: 0.01\n",
            "EarlyStoppingCounter: 1 out of 10\n",
            "Epoch 5/50, Train Loss: 2.7482, Train Acc: 0.3023, Val Loss: 2.8638, Val Acc: 0.2939, Time: 171.67s, Last leraning rate: 0.01\n",
            "EarlyStoppingCounter: 2 out of 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5d9a5657c0a5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ultimate_cnn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m best_val_acc, test_acc = train_and_evaluate(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-1c01f4cd0b7f>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model_class, name, train_loader, val_loader, test_loader, num_classes, model_path, num_epochs, lr, wd, device, stopping_patience, stopping_value, scheduler_check, WANDB)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping_patience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWANDB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# Load the best model for final evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-1c01f4cd0b7f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, model_name, model_path, stopping_patience, stopping_value, scheduler_check, WANDB)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtrain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Ciclo sui modelli da testare su una cpu capace -> uso data augmentation, learning scheduler e batch_size = 32\n",
        "model = [MoreFullyConnectedNet, NetWithoutBatchNorm, NetWithoutResidualBlock, NetWihtoutDropout, GroceryConvNet]\n",
        "model_name = ['NetMoreFullyConnected.cnn', 'NetWithoutBatchNorm.cnn', 'NetWithoutResidualBlock.cnn', 'NetWihtoutDropout.cnn', 'GroceryConvNet.cnn']\n",
        "experiments = zip(model, model_name)\n",
        "\n",
        "for run in range(1,4): #-> se volessi fare più run ma per questioni tempo-gpu la escluderei\n",
        "  for exp in experiments:\n",
        "    model, model_name = exp\n",
        "    model_path = create_model_directory(model_name= model_name, run=run)\n",
        "    best_val_acc, test_acc = train_and_evaluate(\n",
        "    model_class = model,\n",
        "    name = name,\n",
        "    train_loader = increased_train_dl,\n",
        "    val_loader = val_dl,\n",
        "    test_loader = test_dl,\n",
        "    num_classes = n_classes,\n",
        "    model_path = model_path,\n",
        "    num_epochs=cfg_hyp['epochs'],\n",
        "    lr=cfg_hyp['lr'],\n",
        "    wd=cfg_hyp['wd'],\n",
        "    device=cfg_hyp['device'],\n",
        "    stopping_patience = cfg_hyp['stopping_patience'],\n",
        "    stopping_value = cfg_hyp['stopping_value'],\n",
        "    scheduler_check=True,\n",
        "    WANDB=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "k_WF0Y4kyfMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Iperparametri mi sa che ha senso poi farlo manualmente\n",
        "# - Impostando scheduler come False (partendo lr di 1e-3)\n",
        "# - Passando train_dl\n",
        "# - Cambiando il batch size (16, 64) ???\n",
        "\n"
      ],
      "metadata": {
        "id": "lD0p2LTNz24o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COSA RESTA DA FARE:\n",
        "- Testare sulle varie reti plottando i vari grafici (farlo in locale sarebbe fighissimo, capiamo)\n",
        "- Sulla rete migliore variare gli iperparametri (come no o si scheduler o per leraning rate o comunque qualcosa legato al learning rate, batch_size, si e no data augmentation)\n",
        "\n",
        "-> così in totale avrei 5 grafici per le reti e 4 per gli iperparametri\n",
        "\n",
        "- Migliorare questione scheduler perchè effettivamente anche con true non so se sta funzionando\n",
        "\n",
        "- Questione bilanciamento del dataset\n",
        "\n",
        "- Usare nn.Sequential (?) così necessario (?)\n",
        "\n",
        "Alla fine meglio fare ablation study per capire sia riguardo gli iperparamentri che la rete quale è il milgioramento più efficace"
      ],
      "metadata": {
        "id": "0A7dzY-Bmy9p"
      }
    }
  ]
}